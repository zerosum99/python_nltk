{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 품사의 태깅은 문장에 있는 단어가 명사, 동사, 형용사 등으로 식별\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 품사 태킹을 위한 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nltk.tag' from '/Users/dahlmoon/anaconda/lib/python3.6/site-packages/nltk/tag/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffixTagger\n",
      "BigramTagger\n",
      "BrillTagger\n",
      "BrillTaggerTrainer\n",
      "CRFTagger\n",
      "ClassifierBasedPOSTagger\n",
      "ClassifierBasedTagger\n",
      "ContextTagger\n",
      "DefaultTagger\n",
      "HiddenMarkovModelTagger\n",
      "HiddenMarkovModelTrainer\n",
      "HunposTagger\n",
      "NgramTagger\n",
      "PerceptronTagger\n",
      "RUS_PICKLE\n",
      "RegexpTagger\n",
      "SennaChunkTagger\n",
      "SennaNERTagger\n",
      "SennaTagger\n",
      "SequentialBackoffTagger\n",
      "StanfordNERTagger\n",
      "StanfordPOSTagger\n",
      "StanfordTagger\n",
      "TaggerI\n",
      "TnT\n",
      "TrigramTagger\n",
      "UnigramTagger\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for i in dir(nltk.tag) :\n",
    "    if re.match(\"[A-Z]\",i[0]) :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate\n",
      "tag\n",
      "tag_sents\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import * \n",
    "\n",
    "for i in dir(TaggerI) :\n",
    "    if not i.startswith(\"_\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. 문장을 단어로 토큰화해서 처리 하고 이를 기반으로 품사로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 품사로 분한 것을 문자열을 튜플로 튜플을 문자열로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Neither', 'CC'), ('he', 'PPS'), ('or', 'CC'), ('she', 'PPS'), ('likes', 'VB'), ('skiing', 'VBN')]\n",
      "Neither/CC he/PPS or/CC she/PPS likes/VB skiing/VBN\n"
     ]
    }
   ],
   "source": [
    "import nltk,re\n",
    "\n",
    "text = \"Neither/CC he/PPS or/CC she/PPS likes/VB skiing/VBN\"\n",
    "\n",
    "b = [nltk.tag.str2tuple(t) for t in text.split()]\n",
    "print(b)\n",
    "\n",
    "\n",
    "c = \" \".join([nltk.tag.tuple2str(t) for t in b ])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 품사에 대해 이해하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## help 패키지 내의 tagset 에 대한 정보 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nltk.help' from '/Users/dahlmoon/anaconda/lib/python3.6/site-packages/nltk/help.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_format_tagset',\n",
       " '_print_entries',\n",
       " 'brown_tagset',\n",
       " 'claws5_tagset',\n",
       " 'load',\n",
       " 'print_function',\n",
       " 're',\n",
       " 'upenn_tagset',\n",
       " 'wrap']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk.help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABL: determiner/pronoun, pre-qualifier\n",
      "    quite such rather\n"
     ]
    }
   ],
   "source": [
    "nltk.help.brown_tagset('ABL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. penn Treebank POS tag list:\n",
    "\n",
    "    CC\tcoordinating conjunction\n",
    "    CD\tcardinal digit\n",
    "    DT\tdeterminer\n",
    "    EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "    FW\tforeign word\n",
    "    IN\tpreposition/subordinating conjunction\n",
    "    JJ\tadjective\t'big'\n",
    "    JJR\tadjective, comparative\t'bigger'\n",
    "    JJS\tadjective, superlative\t'biggest'\n",
    "    LS\tlist marker\t1)\n",
    "    MD\tmodal\tcould, will\n",
    "    NN\tnoun, singular 'desk'\n",
    "    NNS\tnoun plural\t'desks'\n",
    "    NNP\tproper noun, singular\t'Harrison'\n",
    "    NNPS\tproper noun, plural\t'Americans'\n",
    "    PDT\tpredeterminer\t'all the kids'\n",
    "    POS\tpossessive ending\tparent's\n",
    "    PRP\tpersonal pronoun\tI, he, she, they\n",
    "    PRP\\$\tpossessive pronoun\tmy, his, hers\n",
    "    RB\tadverb\tvery, silently,\n",
    "    RBR\tadverb, comparative\tbetter\n",
    "    RBS\tadverb, superlative\tbest\n",
    "    RP\tparticle\tgive up\n",
    "    TO\tto\tgo 'to' the store.\n",
    "    UH\tinterjection\terrrrrrrrm\n",
    "    VB\tverb, base form\ttake\n",
    "    VBD\tverb, past tense\ttook\n",
    "    VBG\tverb, gerund/present participle\ttaking\n",
    "    VBN\tverb, past participle\ttaken\n",
    "    VBP\tverb, sing. present, non-3d\ttake\n",
    "    VBZ\tverb, 3rd person sing. present\ttakes\n",
    "    WDT\twh-determiner\twhich\n",
    "    WP\twh-pronoun\twho, what\n",
    "    WP$\tpossessive wh-pronoun\twhose\n",
    "    WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사에 대한 세부 정보 확인 하기 \n",
    "\n",
    "        help.upenn_tagset 함수를 조회하면 세부적인 품사에 대한 정보가 나온다.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 실제 파이썬 클래스들을 저장한 pickle을 로딩해서 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 저장된 디렉토리 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPY3\u001b[m\u001b[m/                  claws5_tagset.pickle\r\n",
      "brown_tagset.pickle   upenn_tagset.pickle\r\n"
     ]
    }
   ],
   "source": [
    "%ls ~/nltk_data/help/tagsets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장된 pickle 을 불러와서 내부 확인하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun, common, singular or mass\n",
      "dict_keys(['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "print(tagdict['NN'][0])\n",
    "\n",
    "\n",
    "print(tagdict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brown에서 가져오기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 단어별로 품사를 리스트 튜플로 처리한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method tagged_words in module nltk.corpus.reader.tagged:\n",
      "\n",
      "tagged_words(fileids=None, categories=None, tagset=None) method of nltk.corpus.reader.tagged.CategorizedTaggedCorpusReader instance\n",
      "    :return: the given file(s) as a list of tagged\n",
      "        words and punctuation symbols, encoded as tuples\n",
      "        ``(word,tag)``.\n",
      "    :rtype: list(tuple(str,str))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.corpus.brown.tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사별로 추출해서 보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 말뭉치 중에 특정 범주로 가져와서 품사 중에 동사에 해당하는 것만 추출해서 빈도수에  맞춰 처리 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accepted/VBN-TL', 'Accounting/VBG', 'Acting/VBG', 'Acting/VBG-TL', 'Add/VB', 'Adjusted/VBN', 'Admitting/VBG', 'Aiding/VBG-TL', 'Allied/VBN-TL', 'Anticipated/VBN', 'Armed/VBN', 'Armed/VBN-TL', 'Arranging/VBG', 'Ask/VB-HL', 'Asked/VBD', 'Asked/VBN', 'Asks/VBZ-HL', 'Assisting/VBG', 'Associated/VBN-TL', 'Assuming/VBG']\n",
      "[('said/VBD', 382), ('produced/VBD', 5), ('took/VBD', 47), ('deserves/VBZ', 3), ('conducted/VBN', 6), ('charged/VBN', 5), ('investigate/VB', 3), ('won/VBN', 6), ('received/VBN', 10), ('find/VB', 16)]\n"
     ]
    }
   ],
   "source": [
    "wt = []\n",
    "brown_a = nltk.corpus.brown.tagged_words(categories=\"news\")\n",
    "for (word, tag) in brown_a:\n",
    "    if tag[:2] == 'VB' :\n",
    "        wt.append(word + '/' + tag)\n",
    "fd=nltk.FreqDist(wt)\n",
    "\n",
    "\n",
    "print(sorted(fd)[:20])\n",
    "\n",
    "print(list(fd.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사만 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$1,000,000,000/NNS', '$1,000/NNS', '$1,500,000/NNS', '$1,500/NNS', '$1,600/NNS', '$1,800/NNS', '$1.1/NNS', '$1.4/NNS', '$1.5/NNS', '$1.80/NNS', '$1/NN', '$10,000-per-year/NNS', '$10,000/NNS', '$10/NNS', '$100,000/NNS', '$100/NNS', '$102,285,000/NNS', '$109/NNS', '$11.50/NNS', '$115,000/NNS']\n"
     ]
    }
   ],
   "source": [
    "nouns = []\n",
    "for (w, t) in brown_a:\n",
    "           if t[:2] == 'NN':\n",
    "                nouns.append(w + '/' + t)\n",
    "nfd = nltk.FreqDist(nouns)\n",
    "\n",
    "print(sorted(nfd)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brown_tagged_sents(genre):\n",
    "    \"\"\"Returns the tagged sentences of the given Brown category.\"\"\"\n",
    "    return nltk.corpus.brown.tagged_sents(categories=genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Thirty-three', 'CD-HL')], [('Scotty', 'NP'), ('did', 'DOD'), ('not', '*'), ('go', 'VB'), ('back', 'RB'), ('to', 'IN'), ('school', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagged_sents(\"fiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## treebank 말뭉치를 이용해서 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConditionalFreqDist with 12408 conditions>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "wsj_tagged = nltk.corpus.treebank.tagged_words() \n",
    "word_tags = nltk.ConditionalFreqDist(wsj_tagged) \n",
    "\n",
    "print(word_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConditionalFreqDist는 기존에 토큰호 된것을 FreqDist로 처리된 값을 가지도록 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ConditionalFreqDist in module nltk.probability:\n",
      "\n",
      "class ConditionalFreqDist(collections.defaultdict)\n",
      " |  A collection of frequency distributions for a single experiment\n",
      " |  run under different conditions.  Conditional frequency\n",
      " |  distributions are used to record the number of times each sample\n",
      " |  occurred, given the condition under which the experiment was run.\n",
      " |  For example, a conditional frequency distribution could be used to\n",
      " |  record the frequency of each word (type) in a document, given its\n",
      " |  length.  Formally, a conditional frequency distribution can be\n",
      " |  defined as a function that maps from each condition to the\n",
      " |  FreqDist for the experiment under that condition.\n",
      " |  \n",
      " |  Conditional frequency distributions are typically constructed by\n",
      " |  repeatedly running an experiment under a variety of conditions,\n",
      " |  and incrementing the sample outcome counts for the appropriate\n",
      " |  conditions.  For example, the following code will produce a\n",
      " |  conditional frequency distribution that encodes how often each\n",
      " |  word type occurs, given the length of that word type:\n",
      " |  \n",
      " |      >>> from nltk.probability import ConditionalFreqDist\n",
      " |      >>> from nltk.tokenize import word_tokenize\n",
      " |      >>> sent = \"the the the dog dog some other words that we do not care about\"\n",
      " |      >>> cfdist = ConditionalFreqDist()\n",
      " |      >>> for word in word_tokenize(sent):\n",
      " |      ...     condition = len(word)\n",
      " |      ...     cfdist[condition][word] += 1\n",
      " |  \n",
      " |  An equivalent way to do this is with the initializer:\n",
      " |  \n",
      " |      >>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))\n",
      " |  \n",
      " |  The frequency distribution for each condition is accessed using\n",
      " |  the indexing operator:\n",
      " |  \n",
      " |      >>> cfdist[3]\n",
      " |      FreqDist({'the': 3, 'dog': 2, 'not': 1})\n",
      " |      >>> cfdist[3].freq('the')\n",
      " |      0.5\n",
      " |      >>> cfdist[3]['dog']\n",
      " |      2\n",
      " |  \n",
      " |  When the indexing operator is used to access the frequency\n",
      " |  distribution for a condition that has not been accessed before,\n",
      " |  ``ConditionalFreqDist`` creates a new empty FreqDist for that\n",
      " |  condition.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConditionalFreqDist\n",
      " |      collections.defaultdict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  N(self)\n",
      " |      Return the total number of sample outcomes that have been\n",
      " |      recorded by this ``ConditionalFreqDist``.\n",
      " |      \n",
      " |      :rtype: int\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Add counts from two ConditionalFreqDists.\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |      Intersection is the minimum of corresponding counts.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, cond_samples=None)\n",
      " |      Construct a new empty conditional frequency distribution.  In\n",
      " |      particular, the count for every sample, under every condition,\n",
      " |      is zero.\n",
      " |      \n",
      " |      :param cond_samples: The samples to initialize the conditional\n",
      " |          frequency distribution with\n",
      " |      :type cond_samples: Sequence of (condition, sample) tuples\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Union is the maximum of value in either of the input counters.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this ``ConditionalFreqDist``.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |      Subtract count, but keep only results with positive counts.\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  conditions(self)\n",
      " |      Return a list of the conditions that have been accessed for\n",
      " |      this ``ConditionalFreqDist``.  Use the indexing operator to\n",
      " |      access the frequency distribution for a given condition.\n",
      " |      Note that the frequency distributions for some conditions\n",
      " |      may contain zero sample outcomes.\n",
      " |      \n",
      " |      :rtype: list\n",
      " |  \n",
      " |  plot(self, *args, **kwargs)\n",
      " |      Plot the given samples from the conditional frequency distribution.\n",
      " |      For a cumulative plot, specify cumulative=True.\n",
      " |      (Requires Matplotlib to be installed.)\n",
      " |      \n",
      " |      :param samples: The samples to plot\n",
      " |      :type samples: list\n",
      " |      :param title: The title for the graph\n",
      " |      :type title: str\n",
      " |      :param conditions: The conditions to plot (default is all)\n",
      " |      :type conditions: list\n",
      " |  \n",
      " |  tabulate(self, *args, **kwargs)\n",
      " |      Tabulate the given samples from the conditional frequency distribution.\n",
      " |      \n",
      " |      :param samples: The samples to plot\n",
      " |      :type samples: list\n",
      " |      :param conditions: The conditions to plot (default is all)\n",
      " |      :type conditions: list\n",
      " |      :param cumulative: A flag to specify whether the freqs are cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.defaultdict:\n",
      " |  \n",
      " |  __copy__(...)\n",
      " |      D.copy() -> a shallow copy of D.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __missing__(...)\n",
      " |      __missing__(key) # Called by __getitem__ for missing key; pseudo-code:\n",
      " |      if self.default_factory is None: raise KeyError((key,))\n",
      " |      self[key] = value = self.default_factory()\n",
      " |      return value\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.defaultdict:\n",
      " |  \n",
      " |  default_factory\n",
      " |      Factory for default value called by __missing__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if D has a key k, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Returns a new dict with keys from iterable and values equal to value.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(...)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-517d4881e91a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "help(nltk.ConditionalFreqDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', FreqDist({'NNP': 1})),\n",
       " ('Vinken', FreqDist({'NNP': 2})),\n",
       " (',', FreqDist({',': 4885})),\n",
       " ('61', FreqDist({'CD': 5})),\n",
       " ('years', FreqDist({'NNS': 115})),\n",
       " ('old', FreqDist({'JJ': 24})),\n",
       " ('will', FreqDist({'MD': 280, 'NN': 1})),\n",
       " ('join', FreqDist({'VB': 4})),\n",
       " ('the', FreqDist({'CD': 1, 'DT': 4038, 'JJ': 5, 'NNP': 1})),\n",
       " ('board', FreqDist({'NN': 30}))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_tags.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NN': 2, 'VB': 8, 'VBN': 6, 'VBP': 2})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tags['run'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연설문을 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1945-Truman.txt',\n",
       " '1946-Truman.txt',\n",
       " '1947-Truman.txt',\n",
       " '1948-Truman.txt',\n",
       " '1949-Truman.txt',\n",
       " '1950-Truman.txt',\n",
       " '1951-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1954-Eisenhower.txt',\n",
       " '1955-Eisenhower.txt',\n",
       " '1956-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1958-Eisenhower.txt',\n",
       " '1959-Eisenhower.txt',\n",
       " '1960-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1962-Kennedy.txt',\n",
       " '1963-Johnson.txt',\n",
       " '1963-Kennedy.txt',\n",
       " '1964-Johnson.txt',\n",
       " '1965-Johnson-1.txt',\n",
       " '1965-Johnson-2.txt',\n",
       " '1966-Johnson.txt',\n",
       " '1967-Johnson.txt',\n",
       " '1968-Johnson.txt',\n",
       " '1969-Johnson.txt',\n",
       " '1970-Nixon.txt',\n",
       " '1971-Nixon.txt',\n",
       " '1972-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1974-Nixon.txt',\n",
       " '1975-Ford.txt',\n",
       " '1976-Ford.txt',\n",
       " '1977-Ford.txt',\n",
       " '1978-Carter.txt',\n",
       " '1979-Carter.txt',\n",
       " '1980-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1982-Reagan.txt',\n",
       " '1983-Reagan.txt',\n",
       " '1984-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1986-Reagan.txt',\n",
       " '1987-Reagan.txt',\n",
       " '1988-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1990-Bush.txt',\n",
       " '1991-Bush-1.txt',\n",
       " '1991-Bush-2.txt',\n",
       " '1992-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1994-Clinton.txt',\n",
       " '1995-Clinton.txt',\n",
       " '1996-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '1998-Clinton.txt',\n",
       " '1999-Clinton.txt',\n",
       " '2000-Clinton.txt',\n",
       " '2001-GWBush-1.txt',\n",
       " '2001-GWBush-2.txt',\n",
       " '2002-GWBush.txt',\n",
       " '2003-GWBush.txt',\n",
       " '2004-GWBush.txt',\n",
       " '2005-GWBush.txt',\n",
       " '2006-GWBush.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장을 분리하는 기준을 넣어서 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x112e7b128>\n"
     ]
    }
   ],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "print(custom_sent_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PunktSentenceTokenizer in module nltk.tokenize.punkt:\n",
      "\n",
      "class PunktSentenceTokenizer(PunktBaseClass, nltk.tokenize.api.TokenizerI)\n",
      " |  A sentence tokenizer which uses an unsupervised algorithm to build\n",
      " |  a model for abbreviation words, collocations, and words that start\n",
      " |  sentences; and then uses that model to find sentence boundaries.\n",
      " |  This approach has been shown to work well for many European\n",
      " |  languages.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PunktSentenceTokenizer\n",
      " |      PunktBaseClass\n",
      " |      nltk.tokenize.api.TokenizerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, train_text=None, verbose=False, lang_vars=<nltk.tokenize.punkt.PunktLanguageVars object at 0x1102d3c88>, token_cls=<class 'nltk.tokenize.punkt.PunktToken'>)\n",
      " |      train_text can either be the sole training text for this sentence\n",
      " |      boundary detector, or can be a PunktParameters object.\n",
      " |  \n",
      " |  debug_decisions(self, text)\n",
      " |      Classifies candidate periods as sentence breaks, yielding a dict for\n",
      " |      each that may be used to understand why the decision was made.\n",
      " |      \n",
      " |      See format_debug_decision() to help make this output readable.\n",
      " |  \n",
      " |  dump(self, tokens)\n",
      " |      # [XX] TESTING\n",
      " |  \n",
      " |  sentences_from_text(self, text, realign_boundaries=True)\n",
      " |      Given a text, generates the sentences in that text by only\n",
      " |      testing candidate sentence breaks. If realign_boundaries is\n",
      " |      True, includes in the sentence closing punctuation that\n",
      " |      follows the period.\n",
      " |  \n",
      " |  sentences_from_text_legacy(self, text)\n",
      " |      Given a text, generates the sentences in that text. Annotates all\n",
      " |      tokens, rather than just those with possible sentence breaks. Should\n",
      " |      produce the same results as ``sentences_from_text``.\n",
      " |  \n",
      " |  sentences_from_tokens(self, tokens)\n",
      " |      Given a sequence of tokens, generates lists of tokens, each list\n",
      " |      corresponding to a sentence.\n",
      " |  \n",
      " |  span_tokenize(self, text, realign_boundaries=True)\n",
      " |      Given a text, returns a list of the (start, end) spans of sentences\n",
      " |      in the text.\n",
      " |  \n",
      " |  text_contains_sentbreak(self, text)\n",
      " |      Returns True if the given text includes a sentence break.\n",
      " |  \n",
      " |  tokenize(self, text, realign_boundaries=True)\n",
      " |      Given a text, returns a list of the sentences in that text.\n",
      " |  \n",
      " |  train(self, train_text, verbose=False)\n",
      " |      Derives parameters from a given training text, or uses the parameters\n",
      " |      given. Repeated calls to this method destroy previous parameters. For\n",
      " |      incremental training, instantiate a separate PunktTrainer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PUNCTUATION = (';', ':', ',', '.', '!', '?')\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from PunktBaseClass:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tokenize.api.TokenizerI:\n",
      " |  \n",
      " |  span_tokenize_sents(self, strings)\n",
      " |      Apply ``self.span_tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.span_tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :rtype: iter(list(tuple(int, int)))\n",
      " |  \n",
      " |  tokenize_sents(self, strings)\n",
      " |      Apply ``self.tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :rtype: list(list(str))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PunktSentenceTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  실제 문장 단위로 토큰을 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\", 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.', 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.', '(Applause.)', 'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.']\n"
     ]
    }
   ],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(tokenized[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어로 토큰화하고 그 품사를 달기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
