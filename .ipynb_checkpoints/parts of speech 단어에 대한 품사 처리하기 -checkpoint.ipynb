{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문자열을 튜플로 튜플을 문자열로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Neither', 'CC'), ('he', 'PPS'), ('or', 'CC'), ('she', 'PPS'), ('likes', 'VB'), ('skiing', 'VBN')]\n",
      "Neither/CC he/PPS or/CC she/PPS likes/VB skiing/VBN\n"
     ]
    }
   ],
   "source": [
    "import nltk,re\n",
    "\n",
    "text = \"Neither/CC he/PPS or/CC she/PPS likes/VB skiing/VBN\"\n",
    "\n",
    "b = [nltk.tag.str2tuple(t) for t in text.split()]\n",
    "print(b)\n",
    "\n",
    "\n",
    "c = \" \".join([nltk.tag.tuple2str(t) for t in b ])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tag list:\n",
    "\n",
    "    CC\tcoordinating conjunction\n",
    "    CD\tcardinal digit\n",
    "    DT\tdeterminer\n",
    "    EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "    FW\tforeign word\n",
    "    IN\tpreposition/subordinating conjunction\n",
    "    JJ\tadjective\t'big'\n",
    "    JJR\tadjective, comparative\t'bigger'\n",
    "    JJS\tadjective, superlative\t'biggest'\n",
    "    LS\tlist marker\t1)\n",
    "    MD\tmodal\tcould, will\n",
    "    NN\tnoun, singular 'desk'\n",
    "    NNS\tnoun plural\t'desks'\n",
    "    NNP\tproper noun, singular\t'Harrison'\n",
    "    NNPS\tproper noun, plural\t'Americans'\n",
    "    PDT\tpredeterminer\t'all the kids'\n",
    "    POS\tpossessive ending\tparent's\n",
    "    PRP\tpersonal pronoun\tI, he, she, they\n",
    "    PRP\\$\tpossessive pronoun\tmy, his, hers\n",
    "    RB\tadverb\tvery, silently,\n",
    "    RBR\tadverb, comparative\tbetter\n",
    "    RBS\tadverb, superlative\tbest\n",
    "    RP\tparticle\tgive up\n",
    "    TO\tto\tgo 'to' the store.\n",
    "    UH\tinterjection\terrrrrrrrm\n",
    "    VB\tverb, base form\ttake\n",
    "    VBD\tverb, past tense\ttook\n",
    "    VBG\tverb, gerund/present participle\ttaking\n",
    "    VBN\tverb, past participle\ttaken\n",
    "    VBP\tverb, sing. present, non-3d\ttake\n",
    "    VBZ\tverb, 3rd person sing. present\ttakes\n",
    "    WDT\twh-determiner\twhich\n",
    "    WP\twh-pronoun\twho, what\n",
    "    WP$\tpossessive wh-pronoun\twhose\n",
    "    WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사에 대한 세부 정보 확인 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장된 pickle 을 불러와서 내부 확인하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun, common, singular or mass\n",
      "dict_keys(['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "print(tagdict['NN'][0])\n",
    "\n",
    "\n",
    "print(tagdict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brown에서 가져오기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 단어별로 품사를 리스트 튜플로 처리한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method tagged_words in module nltk.corpus.reader.tagged:\n",
      "\n",
      "tagged_words(fileids=None, categories=None, tagset=None) method of nltk.corpus.reader.tagged.CategorizedTaggedCorpusReader instance\n",
      "    :return: the given file(s) as a list of tagged\n",
      "        words and punctuation symbols, encoded as tuples\n",
      "        ``(word,tag)``.\n",
      "    :rtype: list(tuple(str,str))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.corpus.brown.tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사별로 추출해서 보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 말뭉치 중에 특정 범주로 가져와서 품사 중에 동사에 해당하는 것만 추출해서 빈도수에  맞춰 처리 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accepted/VBN-TL', 'Accounting/VBG', 'Acting/VBG', 'Acting/VBG-TL', 'Add/VB', 'Adjusted/VBN', 'Admitting/VBG', 'Aiding/VBG-TL', 'Allied/VBN-TL', 'Anticipated/VBN', 'Armed/VBN', 'Armed/VBN-TL', 'Arranging/VBG', 'Ask/VB-HL', 'Asked/VBD', 'Asked/VBN', 'Asks/VBZ-HL', 'Assisting/VBG', 'Associated/VBN-TL', 'Assuming/VBG']\n",
      "[('said/VBD', 382), ('produced/VBD', 5), ('took/VBD', 47), ('deserves/VBZ', 3), ('conducted/VBN', 6), ('charged/VBN', 5), ('investigate/VB', 3), ('won/VBN', 6), ('received/VBN', 10), ('find/VB', 16)]\n"
     ]
    }
   ],
   "source": [
    "wt = []\n",
    "brown_a = nltk.corpus.brown.tagged_words(categories=\"news\")\n",
    "for (word, tag) in brown_a:\n",
    "    if tag[:2] == 'VB' :\n",
    "        wt.append(word + '/' + tag)\n",
    "fd=nltk.FreqDist(wt)\n",
    "\n",
    "\n",
    "print(sorted(fd)[:20])\n",
    "\n",
    "print(list(fd.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사만 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$1,000,000,000/NNS', '$1,000/NNS', '$1,500,000/NNS', '$1,500/NNS', '$1,600/NNS', '$1,800/NNS', '$1.1/NNS', '$1.4/NNS', '$1.5/NNS', '$1.80/NNS', '$1/NN', '$10,000-per-year/NNS', '$10,000/NNS', '$10/NNS', '$100,000/NNS', '$100/NNS', '$102,285,000/NNS', '$109/NNS', '$11.50/NNS', '$115,000/NNS']\n"
     ]
    }
   ],
   "source": [
    "nouns = []\n",
    "for (w, t) in brown_a:\n",
    "           if t[:2] == 'NN':\n",
    "                nouns.append(w + '/' + t)\n",
    "nfd = nltk.FreqDist(nouns)\n",
    "\n",
    "print(sorted(nfd)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brown_tagged_sents(genre):\n",
    "    \"\"\"Returns the tagged sentences of the given Brown category.\"\"\"\n",
    "    return nltk.corpus.brown.tagged_sents(categories=genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Thirty-three', 'CD-HL')], [('Scotty', 'NP'), ('did', 'DOD'), ('not', '*'), ('go', 'VB'), ('back', 'RB'), ('to', 'IN'), ('school', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagged_sents(\"fiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## treebank 말뭉치를 이용해서 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConditionalFreqDist with 12408 conditions>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "wsj_tagged = nltk.corpus.treebank.tagged_words() \n",
    "word_tags = nltk.ConditionalFreqDist(wsj_tagged) \n",
    "\n",
    "print(word_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConditionalFreqDist는 기존에 토큰호 된것을 FreqDist로 처리된 값을 가지도록 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ConditionalFreqDist in module nltk.probability:\n",
      "\n",
      "class ConditionalFreqDist(collections.defaultdict)\n",
      " |  A collection of frequency distributions for a single experiment\n",
      " |  run under different conditions.  Conditional frequency\n",
      " |  distributions are used to record the number of times each sample\n",
      " |  occurred, given the condition under which the experiment was run.\n",
      " |  For example, a conditional frequency distribution could be used to\n",
      " |  record the frequency of each word (type) in a document, given its\n",
      " |  length.  Formally, a conditional frequency distribution can be\n",
      " |  defined as a function that maps from each condition to the\n",
      " |  FreqDist for the experiment under that condition.\n",
      " |  \n",
      " |  Conditional frequency distributions are typically constructed by\n",
      " |  repeatedly running an experiment under a variety of conditions,\n",
      " |  and incrementing the sample outcome counts for the appropriate\n",
      " |  conditions.  For example, the following code will produce a\n",
      " |  conditional frequency distribution that encodes how often each\n",
      " |  word type occurs, given the length of that word type:\n",
      " |  \n",
      " |      >>> from nltk.probability import ConditionalFreqDist\n",
      " |      >>> from nltk.tokenize import word_tokenize\n",
      " |      >>> sent = \"the the the dog dog some other words that we do not care about\"\n",
      " |      >>> cfdist = ConditionalFreqDist()\n",
      " |      >>> for word in word_tokenize(sent):\n",
      " |      ...     condition = len(word)\n",
      " |      ...     cfdist[condition][word] += 1\n",
      " |  \n",
      " |  An equivalent way to do this is with the initializer:\n",
      " |  \n",
      " |      >>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))\n",
      " |  \n",
      " |  The frequency distribution for each condition is accessed using\n",
      " |  the indexing operator:\n",
      " |  \n",
      " |      >>> cfdist[3]\n",
      " |      FreqDist({'the': 3, 'dog': 2, 'not': 1})\n",
      " |      >>> cfdist[3].freq('the')\n",
      " |      0.5\n",
      " |      >>> cfdist[3]['dog']\n",
      " |      2\n",
      " |  \n",
      " |  When the indexing operator is used to access the frequency\n",
      " |  distribution for a condition that has not been accessed before,\n",
      " |  ``ConditionalFreqDist`` creates a new empty FreqDist for that\n",
      " |  condition.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConditionalFreqDist\n",
      " |      collections.defaultdict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  N(self)\n",
      " |      Return the total number of sample outcomes that have been\n",
      " |      recorded by this ``ConditionalFreqDist``.\n",
      " |      \n",
      " |      :rtype: int\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Add counts from two ConditionalFreqDists.\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |      Intersection is the minimum of corresponding counts.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, cond_samples=None)\n",
      " |      Construct a new empty conditional frequency distribution.  In\n",
      " |      particular, the count for every sample, under every condition,\n",
      " |      is zero.\n",
      " |      \n",
      " |      :param cond_samples: The samples to initialize the conditional\n",
      " |          frequency distribution with\n",
      " |      :type cond_samples: Sequence of (condition, sample) tuples\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Union is the maximum of value in either of the input counters.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this ``ConditionalFreqDist``.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |      Subtract count, but keep only results with positive counts.\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  conditions(self)\n",
      " |      Return a list of the conditions that have been accessed for\n",
      " |      this ``ConditionalFreqDist``.  Use the indexing operator to\n",
      " |      access the frequency distribution for a given condition.\n",
      " |      Note that the frequency distributions for some conditions\n",
      " |      may contain zero sample outcomes.\n",
      " |      \n",
      " |      :rtype: list\n",
      " |  \n",
      " |  plot(self, *args, **kwargs)\n",
      " |      Plot the given samples from the conditional frequency distribution.\n",
      " |      For a cumulative plot, specify cumulative=True.\n",
      " |      (Requires Matplotlib to be installed.)\n",
      " |      \n",
      " |      :param samples: The samples to plot\n",
      " |      :type samples: list\n",
      " |      :param title: The title for the graph\n",
      " |      :type title: str\n",
      " |      :param conditions: The conditions to plot (default is all)\n",
      " |      :type conditions: list\n",
      " |  \n",
      " |  tabulate(self, *args, **kwargs)\n",
      " |      Tabulate the given samples from the conditional frequency distribution.\n",
      " |      \n",
      " |      :param samples: The samples to plot\n",
      " |      :type samples: list\n",
      " |      :param conditions: The conditions to plot (default is all)\n",
      " |      :type conditions: list\n",
      " |      :param cumulative: A flag to specify whether the freqs are cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.defaultdict:\n",
      " |  \n",
      " |  __copy__(...)\n",
      " |      D.copy() -> a shallow copy of D.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __missing__(...)\n",
      " |      __missing__(key) # Called by __getitem__ for missing key; pseudo-code:\n",
      " |      if self.default_factory is None: raise KeyError((key,))\n",
      " |      self[key] = value = self.default_factory()\n",
      " |      return value\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.defaultdict:\n",
      " |  \n",
      " |  default_factory\n",
      " |      Factory for default value called by __missing__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if D has a key k, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Returns a new dict with keys from iterable and values equal to value.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(...)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-517d4881e91a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "help(nltk.ConditionalFreqDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', FreqDist({'NNP': 1})),\n",
       " ('Vinken', FreqDist({'NNP': 2})),\n",
       " (',', FreqDist({',': 4885})),\n",
       " ('61', FreqDist({'CD': 5})),\n",
       " ('years', FreqDist({'NNS': 115})),\n",
       " ('old', FreqDist({'JJ': 24})),\n",
       " ('will', FreqDist({'MD': 280, 'NN': 1})),\n",
       " ('join', FreqDist({'VB': 4})),\n",
       " ('the', FreqDist({'CD': 1, 'DT': 4038, 'JJ': 5, 'NNP': 1})),\n",
       " ('board', FreqDist({'NN': 30}))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_tags.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NN': 2, 'VB': 8, 'VBN': 6, 'VBP': 2})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tags['run'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연설문을 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1945-Truman.txt',\n",
       " '1946-Truman.txt',\n",
       " '1947-Truman.txt',\n",
       " '1948-Truman.txt',\n",
       " '1949-Truman.txt',\n",
       " '1950-Truman.txt',\n",
       " '1951-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1954-Eisenhower.txt',\n",
       " '1955-Eisenhower.txt',\n",
       " '1956-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1958-Eisenhower.txt',\n",
       " '1959-Eisenhower.txt',\n",
       " '1960-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1962-Kennedy.txt',\n",
       " '1963-Johnson.txt',\n",
       " '1963-Kennedy.txt',\n",
       " '1964-Johnson.txt',\n",
       " '1965-Johnson-1.txt',\n",
       " '1965-Johnson-2.txt',\n",
       " '1966-Johnson.txt',\n",
       " '1967-Johnson.txt',\n",
       " '1968-Johnson.txt',\n",
       " '1969-Johnson.txt',\n",
       " '1970-Nixon.txt',\n",
       " '1971-Nixon.txt',\n",
       " '1972-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1974-Nixon.txt',\n",
       " '1975-Ford.txt',\n",
       " '1976-Ford.txt',\n",
       " '1977-Ford.txt',\n",
       " '1978-Carter.txt',\n",
       " '1979-Carter.txt',\n",
       " '1980-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1982-Reagan.txt',\n",
       " '1983-Reagan.txt',\n",
       " '1984-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1986-Reagan.txt',\n",
       " '1987-Reagan.txt',\n",
       " '1988-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1990-Bush.txt',\n",
       " '1991-Bush-1.txt',\n",
       " '1991-Bush-2.txt',\n",
       " '1992-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1994-Clinton.txt',\n",
       " '1995-Clinton.txt',\n",
       " '1996-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '1998-Clinton.txt',\n",
       " '1999-Clinton.txt',\n",
       " '2000-Clinton.txt',\n",
       " '2001-GWBush-1.txt',\n",
       " '2001-GWBush-2.txt',\n",
       " '2002-GWBush.txt',\n",
       " '2003-GWBush.txt',\n",
       " '2004-GWBush.txt',\n",
       " '2005-GWBush.txt',\n",
       " '2006-GWBush.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장을 분리하는 기준을 넣어서 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x112e7b128>\n"
     ]
    }
   ],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "print(custom_sent_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PunktSentenceTokenizer in module nltk.tokenize.punkt:\n",
      "\n",
      "class PunktSentenceTokenizer(PunktBaseClass, nltk.tokenize.api.TokenizerI)\n",
      " |  A sentence tokenizer which uses an unsupervised algorithm to build\n",
      " |  a model for abbreviation words, collocations, and words that start\n",
      " |  sentences; and then uses that model to find sentence boundaries.\n",
      " |  This approach has been shown to work well for many European\n",
      " |  languages.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PunktSentenceTokenizer\n",
      " |      PunktBaseClass\n",
      " |      nltk.tokenize.api.TokenizerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, train_text=None, verbose=False, lang_vars=<nltk.tokenize.punkt.PunktLanguageVars object at 0x1102d3c88>, token_cls=<class 'nltk.tokenize.punkt.PunktToken'>)\n",
      " |      train_text can either be the sole training text for this sentence\n",
      " |      boundary detector, or can be a PunktParameters object.\n",
      " |  \n",
      " |  debug_decisions(self, text)\n",
      " |      Classifies candidate periods as sentence breaks, yielding a dict for\n",
      " |      each that may be used to understand why the decision was made.\n",
      " |      \n",
      " |      See format_debug_decision() to help make this output readable.\n",
      " |  \n",
      " |  dump(self, tokens)\n",
      " |      # [XX] TESTING\n",
      " |  \n",
      " |  sentences_from_text(self, text, realign_boundaries=True)\n",
      " |      Given a text, generates the sentences in that text by only\n",
      " |      testing candidate sentence breaks. If realign_boundaries is\n",
      " |      True, includes in the sentence closing punctuation that\n",
      " |      follows the period.\n",
      " |  \n",
      " |  sentences_from_text_legacy(self, text)\n",
      " |      Given a text, generates the sentences in that text. Annotates all\n",
      " |      tokens, rather than just those with possible sentence breaks. Should\n",
      " |      produce the same results as ``sentences_from_text``.\n",
      " |  \n",
      " |  sentences_from_tokens(self, tokens)\n",
      " |      Given a sequence of tokens, generates lists of tokens, each list\n",
      " |      corresponding to a sentence.\n",
      " |  \n",
      " |  span_tokenize(self, text, realign_boundaries=True)\n",
      " |      Given a text, returns a list of the (start, end) spans of sentences\n",
      " |      in the text.\n",
      " |  \n",
      " |  text_contains_sentbreak(self, text)\n",
      " |      Returns True if the given text includes a sentence break.\n",
      " |  \n",
      " |  tokenize(self, text, realign_boundaries=True)\n",
      " |      Given a text, returns a list of the sentences in that text.\n",
      " |  \n",
      " |  train(self, train_text, verbose=False)\n",
      " |      Derives parameters from a given training text, or uses the parameters\n",
      " |      given. Repeated calls to this method destroy previous parameters. For\n",
      " |      incremental training, instantiate a separate PunktTrainer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PUNCTUATION = (';', ':', ',', '.', '!', '?')\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from PunktBaseClass:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tokenize.api.TokenizerI:\n",
      " |  \n",
      " |  span_tokenize_sents(self, strings)\n",
      " |      Apply ``self.span_tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.span_tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :rtype: iter(list(tuple(int, int)))\n",
      " |  \n",
      " |  tokenize_sents(self, strings)\n",
      " |      Apply ``self.tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :rtype: list(list(str))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PunktSentenceTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  실제 문장 단위로 토큰을 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\", 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.', 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.', '(Applause.)', 'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.']\n"
     ]
    }
   ],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(tokenized[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어로 토큰화하고 그 품사를 달기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
